Summary of Neural Network Training Procedure and Hyperparameters:

1. Training Procedure:
   - The neural network was trained for conditional codec language modeling.
   - Two conditional language models were used in a hierarchical manner:
     - An autoregressive (AR) decoder-only language model for the first quantizer.
     - A non-autoregressive (NAR) language model for the second to the last quantizers.
   - The AR model was conditioned on the phoneme sequence and the acoustic prompt.
   - The NAR model used the acoustic prompt matrix to constrain speaker identity.
   - Ablation experiments were conducted to study the NAR model with different prompt settings.

2. Hyperparameters:
   - Training was performed using 16 NVIDIA TESLA V100 32GB GPUs.
   - Batch size: 6k acoustic tokens per GPU.
   - Training steps: 800k steps.
   - Optimizer: AdamW.
   - Learning rate scheduling: Warm-up for the first 32k updates to a peak of 5×10−4, then linear decay.
   - Ablation experiments included training NAR models with different numbers of prompts, such as no prompt, phoneme prompt only, and both phoneme and acoustic token prompts.

Overall, the training procedure involved a specific architecture for conditional codec language modeling with a hierarchical approach using AR and NAR models. The hyperparameters included details on batch size, optimizer, learning rate scheduling, and the use of multiple GPUs for training.