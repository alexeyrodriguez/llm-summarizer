Summary:
The paper introduces VALL-E, a neural codec language model for text to speech synthesis (TTS) that leverages large and diverse data to address the zero-shot TTS problem. VALL-E uses discrete codes derived from an audio codec model and treats TTS as a conditional language modeling task. By training on 60K hours of English speech data with over 7000 unique speakers, VALL-E demonstrates strong in-context learning capabilities and the ability to synthesize high-quality personalized speech with minimal speaker enrollment. Experimental results show that VALL-E outperforms existing zero-shot TTS systems in terms of speech naturalness, speaker similarity, and emotion preservation. The model generates diverse synthesized results by conditioning on acoustic tokens from both the speaker and content prompts. Overall, VALL-E represents a significant advancement in zero-shot TTS technology by utilizing large-scale data and innovative modeling techniques.