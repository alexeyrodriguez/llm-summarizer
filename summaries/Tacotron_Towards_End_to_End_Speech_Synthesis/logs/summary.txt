
Please consider the machine learning paper provided in the context below
and provide an overall summary for it.


Context:
Page 0:
TACOTRON : T OWARDS END-TO-ENDSPEECH SYN-
THESIS
Yuxuan Wang∗, RJ Skerry-Ryan∗, Daisy Stanton, Yonghui Wu, Ron J. Weiss†, Navdeep Jaitly,
Zongheng Yang, Ying Xiao∗, Zhifeng Chen, Samy Bengio†, Quoc Le, Yannis Agiomyrgiannakis,
Rob Clark, Rif A. Saurous∗
Google, Inc.
{yxwang,rjryan,rif }@google.com
ABSTRACT
A text-to-speech synthesis system typically consists of multiple stages, such as a
text analysis frontend, an acoustic model and an audio synthesis module. Build-
ing these components often requires extensive domain expertise and may contain
brittle design choices. In this paper, we present Tacotron, an end-to-end genera-
tive text-to-speech model that synthesizes speech directly from characters. Given
<text, audio >pairs, the model can be trained completely from scratch with ran-
dom initialization. We present several key techniques to make the sequence-to-
sequence framework perform well for this challenging task. Tacotron achieves a
3.82 subjective 5-scale mean opinion score on US English, outperforming a pro-
duction parametric system in terms of naturalness. In addition, since Tacotron
generates speech at the frame level, it’s substantially faster than sample-level au-
toregressive methods.
1 I NTRODUCTION
Modern text-to-speech (TTS) pipelines are complex (Taylor, 2009). For example, it is common for
statistical parametric TTS to have a text frontend extracting various linguistic features, a duration
model, an acoustic feature prediction model and a complex signal-processing-based vocoder (Zen
et al., 2009; Agiomyrgiannakis, 2015). These components are based on extensive domain expertise
and are laborious to design. They are also trained independently, so errors from each component
may compound. The complexity of modern TTS designs thus leads to substantial engineering efforts
when building a new system.
There are thus many advantages of an integrated end-to-end TTS system that can be trained on <text,
audio>pairs with minimal human annotation. First, such a system alleviates the need for laborious
feature engineering, which may involve heuristics and brittle design choices. Second, it more easily
allows for rich conditioning on various attributes, such as speaker or language, or high-level features
like sentiment. This is because conditioning can occur at the very beginning of the model rather
than only on certain components. Similarly, adaptation to new data might also be easier. Finally,
a single model is likely to be more robust than a multi-stage model where each component’s errors
can compound. These advantages imply that an end-to-end model could allow us to train on huge
amounts of rich, expressive yet often noisy data found in the real world.
TTS is a large-scale inverse problem: a highly compressed source (text) is “decompressed” into
audio. Since the same text can correspond to different pronunciations or speaking styles, this is a
particularly difﬁcult learning task for an end-to-end model: it must cope with large variations at the
signal level for a given input. Moreover, unlike end-to-end speech recognition (Chan et al., 2016)
∗These authors really like tacos.
†These authors would prefer sushi.
1arXiv:1703.10135v2  [cs.CL]  6 Apr 2017

Page 1:
Attention 
Pre-net CBHG 
Character embeddings Attention 
RNNDecoder 
RNN 
Pre-net Attention 
RNNDecoder 
RNN
Pre-net Attention 
RNNDecoder 
RNN
Pre-net CBHG Linear-scale 
spectrogram 
Seq2seq target 
with r=3 Griffin-Lim reconstruction 
Attention is applied 
to all decoder steps 
<GO> frame Figure 1: Model architecture. The model takes characters as input and outputs the corresponding
raw spectrogram, which is then fed to the Grifﬁn-Lim reconstruction algorithm to synthesize speech.
or machine translation (Wu et al., 2016), TTS outputs are continuous, and output sequences are
usually much longer than those of the input. These attributes cause prediction errors to accumulate
quickly. In this paper, we propose Tacotron, an end-to-end generative TTS model based on the
sequence-to-sequence (seq2seq) (Sutskever et al., 2014) with attention paradigm (Bahdanau et al.,
2014). Our model takes characters as input and outputs raw spectrogram, using several techniques
to improve the capability of a vanilla seq2seq model. Given <text, audio >pairs, Tacotron can
be trained completely from scratch with random initialization. It does not require phoneme-level
alignment, so it can easily scale to using large amounts of acoustic data with transcripts. With a
simple waveform synthesis technique, Tacotron produces a 3.82 mean opinion score (MOS) on an
US English eval set, outperforming a production parametric system in terms of naturalness1.
2 R ELATED WORK
WaveNet (van den Oord et al., 2016) is a powerful generative model of audio. It works well for TTS,
but is slow due to its sample-level autoregressive nature. It also requires conditioning on linguistic
features from an existing TTS frontend, and thus is not end-to-end: it only replaces the vocoder and
acoustic model. Another recently-developed neural model is DeepV oice (Arik et al., 2017), which
replaces every component in a typical TTS pipeline by a corresponding neural network. However,
each component is independently trained, and it’s nontrivial to change the system to train in an
end-to-end fashion.
To our knowledge, Wang et al. (2016) is the earliest work touching end-to-end TTS using seq2seq
with attention. However, it requires a pre-trained hidden Markov model (HMM) aligner to help the
seq2seq model learn the alignment. It’s hard to tell how much alignment is learned by the seq2seq
per se. Second, a few tricks are used to get the model trained, which the authors note hurts prosody.
Third, it predicts vocoder parameters hence needs a vocoder. Furthermore, the model is trained on
phoneme inputs and the experimental results seem to be somewhat limited.
Char2Wav (Sotelo et al., 2017) is an independently-developed end-to-end model that can be trained
on characters. However, Char2Wav still predicts vocoder parameters before using a SampleRNN
neural vocoder (Mehri et al., 2016), whereas Tacotron directly predicts raw spectrogram. Also, their
seq2seq and SampleRNN models need to be separately pre-trained, but our model can be trained
1Sound demos can be found at https://google.github.io/tacotron
2

