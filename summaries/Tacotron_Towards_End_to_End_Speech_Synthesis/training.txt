Summary of Neural Network Training Procedure and Hyperparameters:

- The neural network was trained using a batch size of 32.
- An Adam optimizer was used with a learning rate that started at 0.001 and decayed to 0.0005, 0.0003, and 0.0001 after specific global steps.
- The training process employed a simple â„“1 loss for both the seq2seq decoder and post-processing net, with equal weights assigned to the two losses.
- The model architecture included a vanilla seq2seq model with 2 layers of residual RNNs, each layer having 256 GRU cells.
- Scheduled sampling with a sampling rate of 0.5 was utilized to help the model learn alignments and generalize.
- The decoder used a content-based tanh attention mechanism, with the target set as an 80-band mel-scale spectrogram.
- Multiple, non-overlapping output frames were predicted at each decoder step to enhance convergence speed.