
Please consider the machine learning paper provided in the context below
and provide an overall summary for it.


Context:
Page 0:
Speak, Read and Prompt: High-Fidelity Text-to-Speech
with Minimal Supervision
Eugene Kharitonov Damien Vincent Zalán Borsos Raphaël Marinier
Sertan Girgin Olivier Pietquin Matt Shariﬁ Marco Tagliasacchi Neil Zeghidour
Google Research
{kharitonov,damienv,neilz}@google.com
Abstract
We introduce SPEAR-TTS, a multi-speaker
text-to-speech (TTS) system that can be
trained with minimal supervision. By com-
bining two types of discrete speech represen-
tations, we cast TTS as a composition of two
sequence-to-sequence tasks: from text to high-
level semantic tokens (akin to “reading”) and
from semantic tokens to low-level acoustic
tokens (“speaking”). Decoupling these two
tasks enables training of the “speaking” mod-
ule using abundant audio-only data, and un-
locks the highly efﬁcient combination of pre-
training and backtranslation to reduce the need
for parallel data when training the “reading”
component. To control the speaker identity,
we adopt example prompting, which allows
SPEAR-TTS to generalize to unseen speak-
ers using only a short sample of 3 seconds,
without any explicit speaker representation or
speaker-id labels. Our experiments demon-
strate that SPEAR-TTS achieves a character
error rate that is competitive with state-of-the-
art methods using only 15 minutes of parallel
data, while matching ground-truth speech in
terms of naturalness and acoustic quality, as
measured in subjective tests.
1 Introduction
Training a text-to-speech (TTS) system typically
requires hundreds of hours of parallel data in the
form of transcribed utterances. As a consequence,
TTS is only available for “high-resource” lan-
guages. Moreover, the audio generated by such
systems is only as diverse as the parallel data that
they are trained on, which should contain many
speakers, with various accents, of diverse demo-
graphics, and heterogeneous recording conditions.
At the same time, for most languages, including
low-resource ones, audio-only speech data can be
relatively abundant online, present in the forms of
audiobooks, podcasts, radio and TV shows.
In this paper, we investigate how audio-only
data can be leveraged to reduce the need forsupervision in training TTS systems. We introduce
SPEAR-TTS,1a multi-speaker TTS system that
can be trained with as little as 15 minutes of
parallel data from a single speaker. Moreover,
SPEAR-TTS can synthesize a new voice using
only 3 seconds of speech, without any speaker
labels or explicit speaker representation. At its
core, SPEAR-TTS leverages recent advances in the
“textless” modeling of spoken language (Lakhotia
et al., 2021; Dunbar et al., 2021; Polyak et al.,
2021; Kreuk et al., 2021; Kharitonov et al., 2022;
Nguyen et al., 2022; Borsos et al., 2022). These
methods represent continuous audio waveforms
as sequences of tokens from a ﬁnite vocabulary,
casting speech generation as a language modeling
task. In particular, AudioLM (Borsos et al., 2022)
combines two types of discrete tokens: high-level
semantic tokens and low-level acoustic tokens,
which that can be mapped to audio. Using these
representations, we cast the TTS problem as
a “translation” from text transcripts to acoustic
tokens with semantic token representations serving
as a pivot “language” (Utiyama and Isahara, 2007).
This way, TTS is reduced to a composition of two
sequence-to-sequence (seq2seq) tasks: translating
text to semantic tokens, and translating semantic
to acoustic tokens.
The key beneﬁt of splitting the TTS task into
these two sub-tasks is that the supervision needed
to learn how to map text into the intermediate
semantic token representation (“reading”) and how
to produce speech from it (“speaking”) become
decoupled. While the “reading” stage relies on
parallel text-audio data, the audio tokens used to
train the “speaking” component are produced by
self-supervised audio models and therefore can
be extracted from a massive amount of unlabeled
speech data. As a result, the quality and diversity
of the generated speech become independent from
the available parallel data.
1SPEAR stands for “ speak, r ead and p rompt”.
1arXiv:2302.03540v1  [cs.SD]  7 Feb 2023

Page 1:
Casting each stage of SPEAR-TTS as a seq2seq
problem allows us to use standard Transformer
models (Vaswani et al., 2017) and makes it easy
to tap into the vast pool of ideas developed by
the machine translation community to reduce the
need for supervision. Speciﬁcally, we combine
BART/T5-style pretraining (Lewis et al., 2020;
Raffel et al., 2020) with backtranslation (Sennrich
et al., 2016) to signiﬁcantly reduce the amount of
parallel supervision required to train SPEAR-TTS.
To control the voice used by SPEAR-TTS when
producing an utterance, we leverage an example
prompting mechanism that is closely related to
prompting in textual language models (Brown et al.,
2020). Here we condition the “speaking” model
with an audio clip representing the target voice,
steering it to use this voice when generating the
utterance. This feature can simplify building con-
trollable multi-speaker TTS systems for languages
where only single-speaker parallel data is available.
Modeling speech synthesis with seq2seq mod-
els enables using stochastic sampling at inference,
which allows generating outputs of diverse quality
for the same input. We exploit that to improve the
synthesized audio quality by proposing a sampling
scheme based on an objective quality metric.
Our experimental study on English speech
shows that, by combining pretraining and back-
translation over a large dataset — 551 hours
from LibriTTS (Zen et al., 2019) — with just 15
minutes of parallel data from a single speaker —
LJSpeech (Ito and Johnson, 2017) — SPEAR-TTS
(a) generates speech with high ﬁdelity to the
input transcript — CER 1.92% on LibriSpeech
test-clean (Panayotov et al., 2015)); (b) synthesizes
speech with diverse voices, (c) reliably reproduces
the voice of an unseen speaker, when using a 3 sec-
ond example from the target speaker; (d) achieves
high acoustic quality, comparable to that of the
ground-truth utterances (MOS 4.96 vs. 4.92).2
Overall, our approach to building TTS using
massive self-supervised pretraining and back-
translation of discrete speech representations
considerably differs from how existing TTS
systems are implemented (Shen et al., 2018; Kong
et al., 2020; Ren et al., 2020; Kim et al., 2021;
Ao et al., 2022; Wang et al., 2023), signiﬁcantly
reducing the costs related to data collection and
potentially providing high-quality multi-speaker
2Samples produced by SPEAR-TTS can be found on
the demo site: https://google-research.github.io/
seanet/speartts/examples/ .TTS for languages that are not well covered today.
2 Discrete Speech Representations
Below we provide a brief overview of the self-
supervised audio representations that are essential
for SPEAR-TTS. The combined use of these repre-
sentations was proposed in AudioLM (Borsos et al.,
2022), which we refer to for a detailed discussion.
Semantic tokens The role of semantic tokens
is to provide a coarse, high-level conditioning to
subsequently produce acoustic tokens. Thus, they
should provide a representation of speech where
linguistic content — from phonetics to semantics —
is salient, while paralinguistic information such as
speaker identity and acoustic details are removed.
To obtain such a representation, we train a self-
supervised speech representation model based on
w2v-BERT (Chung et al., 2021). This model com-
bines masked language modeling (Devlin et al.,
2019) and contrastive learning (van den Oord et al.,
2018) to obtain speech representations. After its
training, we run a k-means clustering on the mean-
variance normalized outputs of a speciﬁc layer. We
use the centroid indices as discrete tokens.
Acoustic tokens Acoustic tokens are discrete au-
dio representations that provide high-ﬁdelity re-
construction of the acoustic details. We train a
SoundStream (Zeghidour et al., 2021) neural codec
to reconstruct speech while compressing it into few
discrete units. SoundStream achieves this goal by
adding a residual quantizer to the bottleneck of a
convolutional autoencoder. To represent the hierar-
chy of residual quantizers in a sequence, we ﬂatten
the tokens corresponding to the different levels by
interleaving them (Borsos et al., 2022).
3 SPEAR-TTS Overview
SPEAR-TTS extends AudioLM (Borsos et al.,
2022) by enabling text as a form of conditioning.
SPEAR-TTS is organized in two main stages,
as illustrated in Figure 1. In the ﬁrst stage
(S1), text inputs are translated into a sequence
of discrete semantic tokens. The second stage
(S2) maps semantic tokens into acoustic tokens,
which are decoded to speech by the SoundStream
decoder (Zeghidour et al., 2021). This way, S1
learns to map text to the internal representation
provided by semantic tokens (“reading”), while
S2handles the production of speech from this
intermediate internal representation (“speaking”).
2

