Neural Network Training Details:

- Training involved random cropping to equivalent input lengths of 30, 10, and 3 seconds for the three stages.
- Consecutive repetitions of semantic tokens were removed in the first two stages.
- Each stage was trained on 16 TPUv4s with a batch size of 256 for 1M steps.
- The model achieved 98.6% accuracy on a balanced evaluation set.
- Hyperparameters were identical to the speech continuation setup, except for the acoustic generation stage.
- The acoustic generation stage used a codec with 3 layers of quantization and a codebook size of 214 per layer.
- Experiments on piano continuation ignored the third stage and directly predicted the 3 levels of acoustic tokens in the second stage.