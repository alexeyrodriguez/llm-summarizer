Summary:
The paper introduces AudioLM, a framework for high-quality audio generation with long-term consistency. It approaches audio generation as a language modeling task by mapping input audio to a sequence of discrete tokens. By combining semantic tokens from a pre-trained language model and acoustic tokens from a neural audio codec, AudioLM achieves both reconstruction quality and long-term structure. The model can generate natural and coherent audio continuations, such as speech and piano music, without the need for textual annotations. It maintains speaker identity and prosody for unseen speakers and demonstrates the ability to generate coherent music continuations. AudioLM addresses the challenges of high-quality audio synthesis by leveraging recent advances in adversarial neural audio compression, self-supervised representation learning, and language modeling. The framework combines semantic and acoustic tokens hierarchically to achieve long-term consistency and high quality in audio generation.