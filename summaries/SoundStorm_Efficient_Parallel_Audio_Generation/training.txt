### Neural Network Training Details:

1. **Masking and Decoding Design**:
   - Extends MaskGIT scheme to token sequences produced by RVQ.
   - Follows a coarse-to-fine order per RVQ level.
   - Masks tokens for training based on a specific scheme.
   - Sampling of prompt delimiter timestep and RVQ level.

2. **Model Training on LibriLight**:
   - Dataset: LibriLight (60k hours).
   - Training: 10 epochs.
   - Data Processing: Random windows of length 0-30 seconds.

3. **SoundStorm Model Training**:
   - Trained on dialogue corpus for 10 epochs.
   - Includes training of text-to-semantic model.

4. **Model Specifics**:
   - SoundStream codec: 24 kHz, 50 frames per second, 12 RVQ.
   - ByT5-large Transformer: 1.2B parameters, temperature sampling (0.9), top-k (125).

5. **Evaluation Focus**:
   - Comparison with AudioLM's acoustic generator.
   - Metrics: WER, CER, voice preservation, acoustic consistency.

Note: Batch size, learning rate, and specific losses are not provided in the context.