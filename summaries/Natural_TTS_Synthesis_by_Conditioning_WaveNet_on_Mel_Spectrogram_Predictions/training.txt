Neural Network Training Details:

- Feature Prediction Network Training:
  - Batch Size: 64
  - Optimizer: Adam
  - Initial Learning Rate: 10^-3
  - Learning Rate Schedule: Exponential decay to 10^-5 after 50,000 iterations
  - Regularization: L2 with weight 10^-6

- Modified WaveNet Training:
  - Batch Size: 128 (distributed across 32 GPUs)
  - Optimizer: Adam
  - Learning Rate: 10^-4
  - Parameter Update: Exponentially-weighted moving average with decay of 0.9999
  - Target Scaling: Waveform targets scaled by a factor of 127.5

Note: The paper does not provide specific details on the number of epochs or steps, or losses used during training.