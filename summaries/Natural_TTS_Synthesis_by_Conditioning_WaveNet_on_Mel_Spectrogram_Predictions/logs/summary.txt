
Please consider the machine learning paper provided in the context below
and provide an overall summary for it.


Context:
Page 0:
NATURAL TTS SYNTHESIS BY CONDITIONING WA VENET ON MEL SPECTROGRAM
PREDICTIONS
Jonathan Shen1, Ruoming Pang1, Ron J. Weiss1, Mike Schuster1, Navdeep Jaitly1, Zongheng Yang∗2,
Zhifeng Chen1, Yu Zhang1, Yuxuan Wang1, RJ Skerry-Ryan1, Rif A. Saurous1, Yannis Agiomyrgiannakis1,
and Yonghui Wu1
1Google, Inc.,2University of California, Berkeley,
{jonathanasdf,rpang,yonghui }@google.com
ABSTRACT
This paper describes Tacotron 2, a neural network architecture for
speech synthesis directly from text. The system is composed of a
recurrent sequence-to-sequence feature prediction network that maps
character embeddings to mel-scale spectrograms, followed by a mod-
iﬁed WaveNet model acting as a vocoder to synthesize time-domain
waveforms from those spectrograms. Our model achieves a mean
opinion score (MOS) of 4.53comparable to a MOS of 4.58for profes-
sionally recorded speech. To validate our design choices, we present
ablation studies of key components of our system and evaluate the im-
pact of using mel spectrograms as the conditioning input to WaveNet
instead of linguistic, duration, and F0features. We further show that
using this compact acoustic intermediate representation allows for a
signiﬁcant reduction in the size of the WaveNet architecture.
Index Terms —Tacotron 2, WaveNet, text-to-speech
1. INTRODUCTION
Generating natural speech from text (text-to-speech synthesis, TTS)
remains a challenging task despite decades of investigation [ 1]. Over
time, different techniques have dominated the ﬁeld. Concatenative
synthesis with unit selection, the process of stitching small units
of pre-recorded waveforms together [ 2,3] was the state-of-the-art
for many years. Statistical parametric speech synthesis [ 4,5,6,7],
which directly generates smooth trajectories of speech features to be
synthesized by a vocoder, followed, solving many of the issues that
concatenative synthesis had with boundary artifacts. However, the
audio produced by these systems often sounds mufﬂed and unnatural
compared to human speech.
WaveNet [ 8], a generative model of time domain waveforms, pro-
duces audio quality that begins to rival that of real human speech and
is already used in some complete TTS systems [ 9,10,11]. The inputs
to WaveNet (linguistic features, predicted log fundamental frequency
(F0), and phoneme durations), however, require signiﬁcant domain
expertise to produce, involving elaborate text-analysis systems as
well as a robust lexicon (pronunciation guide).
Tacotron [ 12], a sequence-to-sequence architecture [ 13] for pro-
ducing magnitude spectrograms from a sequence of characters, sim-
pliﬁes the traditional speech synthesis pipeline by replacing the pro-
duction of these linguistic and acoustic features with a single neural
network trained from data alone. To vocode the resulting magnitude
spectrograms, Tacotron uses the Grifﬁn-Lim algorithm [ 14] for phase
estimation, followed by an inverse short-time Fourier transform. As
∗Work done while at Google.the authors note, this was simply a placeholder for future neural
vocoder approaches, as Grifﬁn-Lim produces characteristic artifacts
and lower audio quality than approaches like WaveNet.
In this paper, we describe a uniﬁed, entirely neural approach to
speech synthesis that combines the best of the previous approaches:
a sequence-to-sequence Tacotron-style model [ 12] that generates mel
spectrograms, followed by a modiﬁed WaveNet vocoder [ 10,15].
Trained directly on normalized character sequences and correspond-
ing speech waveforms, our model learns to synthesize natural sound-
ing speech that is difﬁcult to distinguish from real human speech.
Deep V oice 3 [ 11] describes a similar approach. However, unlike
our system, its naturalness has not been shown to rival that of human
speech. Char2Wav [ 16] describes yet another similar approach to
end-to-end TTS using a neural vocoder. However, they use different
intermediate representations (traditional vocoder features) and their
model architecture differs signiﬁcantly.
2. MODEL ARCHITECTURE
Our proposed system consists of two components, shown in Figure 1:
(1) a recurrent sequence-to-sequence feature prediction network with
attention which predicts a sequence of mel spectrogram frames from
an input character sequence, and (2) a modiﬁed version of WaveNet
which generates time-domain waveform samples conditioned on the
predicted mel spectrogram frames.
2.1. Intermediate Feature Representation
In this work we choose a low-level acoustic representation: mel-
frequency spectrograms, to bridge the two components. Using a
representation that is easily computed from time-domain waveforms
allows us to train the two components separately. This representation
is also smoother than waveform samples and is easier to train using a
squared error loss because it is invariant to phase within each frame.
A mel-frequency spectrogram is related to the linear-frequency
spectrogram, i.e., the short-time Fourier transform (STFT) magnitude.
It is obtained by applying a nonlinear transform to the frequency
axis of the STFT, inspired by measured responses from the human
auditory system, and summarizes the frequency content with fewer
dimensions. Using such an auditory frequency scale has the effect of
emphasizing details in lower frequencies, which are critical to speech
intelligibility, while de-emphasizing high frequency details, which
are dominated by fricatives and other noise bursts and generally do
not need to be modeled with high ﬁdelity. Because of these properties,
features derived from the mel scale have been used as an underlying
representation for speech recognition for many decades [17].
1arXiv:1712.05884v2  [cs.CL]  16 Feb 2018

Page 1:
While linear spectrograms discard phase information (and are
therefore lossy), algorithms such as Grifﬁn-Lim [ 14] are capable of
estimating this discarded information, which enables time-domain
conversion via the inverse short-time Fourier transform. Mel spectro-
grams discard even more information, presenting a challenging in-
verse problem. However, in comparison to the linguistic and acoustic
features used in WaveNet, the mel spectrogram is a simpler, lower-
level acoustic representation of audio signals. It should therefore
be straightforward for a similar WaveNet model conditioned on mel
spectrograms to generate audio, essentially as a neural vocoder. In-
deed, we will show that it is possible to generate high quality audio
from mel spectrograms using a modiﬁed WaveNet architecture.
2.2. Spectrogram Prediction Network
As in Tacotron, mel spectrograms are computed through a short-
time Fourier transform (STFT) using a 50 ms frame size, 12.5 ms
frame hop, and a Hann window function. We experimented with a
5 ms frame hop to match the frequency of the conditioning inputs
in the original WaveNet, but the corresponding increase in temporal
resolution resulted in signiﬁcantly more pronunciation issues.
We transform the STFT magnitude to the mel scale using an 80
channel mel ﬁlterbank spanning 125 Hz to 7.6 kHz, followed by log
dynamic range compression. Prior to log compression, the ﬁlterbank
output magnitudes are clipped to a minimum value of 0.01 in order
to limit dynamic range in the logarithmic domain.
The network is composed of an encoder and a decoder with atten-
tion. The encoder converts a character sequence into a hidden feature
representation which the decoder consumes to predict a spectrogram.
Input characters are represented using a learned 512-dimensional
character embedding, which are passed through a stack of 3 convolu-
tional layers each containing 512 ﬁlters with shape 5×1, i.e., where
each ﬁlter spans 5 characters, followed by batch normalization [ 18]
and ReLU activations. As in Tacotron, these convolutional layers
model longer-term context (e.g., N-grams) in the input character
sequence. The output of the ﬁnal convolutional layer is passed into a
single bi-directional [ 19] LSTM [ 20] layer containing 512 units (256
in each direction) to generate the encoded features.
The encoder output is consumed by an attention network which
summarizes the full encoded sequence as a ﬁxed-length context vector
for each decoder output step. We use the location-sensitive attention
from [ 21], which extends the additive attention mechanism [ 22] to
use cumulative attention weights from previous decoder time steps
as an additional feature. This encourages the model to move forward
consistently through the input, mitigating potential failure modes
where some subsequences are repeated or ignored by the decoder.
Attention probabilities are computed after projecting inputs and lo-
cation features to 128-dimensional hidden representations. Location
features are computed using 32 1-D convolution ﬁlters of length 31.
The decoder is an autoregressive recurrent neural network which
predicts a mel spectrogram from the encoded input sequence one
frame at a time. The prediction from the previous time step is ﬁrst
passed through a small pre-net containing 2 fully connected layers
of 256 hidden ReLU units. We found that the pre-net acting as an
information bottleneck was essential for learning attention. The pre-
net output and attention context vector are concatenated and passed
through a stack of 2 uni-directional LSTM layers with 1024 units.
The concatenation of the LSTM output and the attention context
vector is projected through a linear transform to predict the target
spectrogram frame. Finally, the predicted mel spectrogram is passed
through a 5-layer convolutional post-net which predicts a residual
to add to the prediction to improve the overall reconstruction. Each
Character 
Embedding Location 
Sensitive 
Attention 
3 Conv 
Layers Bidirectional 
LSTM Input Text 2 Layer 
Pre-Net 2 LSTM 
Layers Linear 
Projection Linear 
Projection 
Stop Token 5 Conv Layer 
Post-Net 
Mel Spectrogram 
WaveNet 
MoLWaveform 
Samples Fig. 1 . Block diagram of the Tacotron 2 system architecture.
post-net layer is comprised of 512 ﬁlters with shape 5×1with batch
normalization, followed by tanh activations on all but the ﬁnal layer.
We minimize the summed mean squared error (MSE) from before
and after the post-net to aid convergence. We also experimented
with a log-likelihood loss by modeling the output distribution with
a Mixture Density Network [ 23,24] to avoid assuming a constant
variance over time, but found that these were more difﬁcult to train
and they did not lead to better sounding samples.
In parallel to spectrogram frame prediction, the concatenation of
decoder LSTM output and the attention context is projected down
to a scalar and passed through a sigmoid activation to predict the
probability that the output sequence has completed. This “stop token”
prediction is used during inference to allow the model to dynamically
determine when to terminate generation instead of always generating
for a ﬁxed duration. Speciﬁcally, generation completes at the ﬁrst
frame for which this probability exceeds a threshold of 0.5.
The convolutional layers in the network are regularized using
dropout [ 25] with probability 0.5, and LSTM layers are regularized
using zoneout [ 26] with probability 0.1. In order to introduce output
variation at inference time, dropout with probability 0.5 is applied
only to layers in the pre-net of the autoregressive decoder.
In contrast to the original Tacotron, our model uses simpler build-
ing blocks, using vanilla LSTM and convolutional layers in the en-
coder and decoder instead of “CBHG” stacks and GRU recurrent
layers. We do not use a “reduction factor”, i.e., each decoder step
corresponds to a single spectrogram frame.
2.3. WaveNet Vocoder
We use a modiﬁed version of the WaveNet architecture from [ 8] to
invert the mel spectrogram feature representation into time-domain
waveform samples. As in the original architecture, there are 30
dilated convolution layers, grouped into 3 dilation cycles, i.e., the
dilation rate of layer k ( k= 0...29) is2k(mod 10). To work with
the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling
layers are used in the conditioning stack instead of 3 layers.
Instead of predicting discretized buckets with a softmax layer,
we follow PixelCNN++ [ 27] and Parallel WaveNet [ 28] and use a 10-
component mixture of logistic distributions (MoL) to generate 16-bit
samples at 24 kHz. To compute the logistic mixture distribution, the
WaveNet stack output is passed through a ReLU activation followed

